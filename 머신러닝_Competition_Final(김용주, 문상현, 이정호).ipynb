{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pandas_profiling as pp\n",
    "import gc\n",
    "\n",
    "# Visualization, EDA\n",
    "# import missingno as msno\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Korean font\n",
    "#font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "#rc('font', family=font_name)\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import optuna\n",
    "\n",
    "# Modeling\n",
    "from lightgbm import LGBMClassifier\n",
    "# from pycaret.classification import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import joblib\n",
    "import platform\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import Image\n",
    "from scipy.stats.mstats import gmean\n",
    "from tensorflow import keras\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터 파일일때(주석처리)\n",
    "panel = pd.read_csv(os.path.abspath(\"../input\") + '/panel.csv')\n",
    "survey = pd.read_csv(os.path.abspath(\"../input\") + '/survey.csv')\n",
    "response_train = pd.read_csv(os.path.abspath(\"../input\") + '/response_train.csv')\n",
    "response_test = pd.read_csv(os.path.abspath(\"../input\") + '/response_test.csv')\n",
    "\n",
    "# 성별, 지역 값이 Nan 값인 패널 1명 test에도 존재 하지 않아 그냥 drop\n",
    "panel.drop(8315, axis=0, inplace=True)\n",
    "# REGION 컬럼 무의미해서 drop\n",
    "panel.drop(['REGION'], axis=1, inplace=True)\n",
    "\n",
    "# CATEGORIES 컬럼 Nan 값도 많고 무의미해서 drop\n",
    "survey.drop(['CATEGORIES'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (패널 질문 응답수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패널 질문 COUNT(질문의 성의 판단)\n",
    "panel_Questions = panel.iloc[:,4:]\n",
    "\n",
    "panel = panel.iloc[:,:4]\n",
    "panel_Questions_count = []\n",
    "for i in range(panel_Questions.shape[0]):\n",
    "            panel_Questions_count.append(pd.notna(panel_Questions.iloc[i,:].values).sum())\n",
    "\n",
    "# 패널 질문 응답수\n",
    "panel['Questions_count'] = panel_Questions_count\n",
    "\n",
    "train = response_train.merge(panel).merge(survey)\n",
    "test = response_test.merge(panel).merge(survey).sort_values(by='ID')\n",
    "\n",
    "# train 데이터 분할\n",
    "before_train = train.query(\"TIME<='2021-04-01 00:00:00'\")\n",
    "train = train.query(\"TIME>='2021-04-01 00:00:00'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (패널 ID별 누적 포인트)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패널 ID 별 획득 누적 포인트\n",
    "Points_earned = pd.DataFrame(before_train.query('STATUS==1').groupby('userID')['CPI'].agg([('Points_earned',np.sum)])).reset_index()\n",
    "\n",
    "train = train.merge(Points_earned, how='left')\n",
    "train['Points_earned'].fillna(0, inplace = True)\n",
    "test = test.merge(Points_earned, how='left')\n",
    "test['Points_earned'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (일자, 요일, 시간, 시간별 범위, 시간 범위별 응답확률), (리워드 포인트별 범위, 리워드 범위별 응답 확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train, test]\n",
    "\n",
    "for i in data:\n",
    "    i['TIME'] = i['TIME'].astype(\"datetime64\")\n",
    "    \n",
    "    i['DAY'] = i['TIME'].dt.day\n",
    "    \n",
    "    i['WEEKDAY'] = i['TIME'].dt.dayofweek\n",
    "    \n",
    "    i['HOUR'] = i['TIME'].dt.hour\n",
    "    \n",
    "    i['HOUR_range'] = pd.cut(i['HOUR'], 4, labels = ['새벽', '오전', '오후', '야간'], right = True, include_lowest=True)\n",
    "   \n",
    "    i['CPI_range'] = pd.cut(i['CPI'],\n",
    "                         bins = [0, 500, 1000, 1500, 2000, 2500 ],\n",
    "                         labels = ['0~500', '501~1000','1001~1500', '1501~2000', '2001~2500'],\n",
    "                         right = True, include_lowest=True)\n",
    "    \n",
    "\n",
    "\n",
    "WEEKDAY_status_mean = pd.DataFrame(train.groupby('WEEKDAY')['STATUS'].agg([('WEEKDAY_status_mean',np.mean)])).reset_index()\n",
    "\n",
    "HOUR_range_status_mean = pd.DataFrame(train.groupby('HOUR_range')['STATUS'].agg([('HOUR_range_status_mean',np.mean)])).reset_index()\n",
    "\n",
    "CPI_range_status_mean = pd.DataFrame(train.groupby('CPI_range')['STATUS'].agg([('CPI_range_status_mean',np.mean)])).reset_index()\n",
    "\n",
    "train = train.merge(CPI_range_status_mean, how='left').merge(WEEKDAY_status_mean, how='left').merge(HOUR_range_status_mean, how='left')#.merge(IR_range_status_mean, how='left').merge(LOI_range_status_mean, how='left')\n",
    "test = test.merge(CPI_range_status_mean, how='left').merge(WEEKDAY_status_mean, how='left').merge(HOUR_range_status_mean, how='left')#.merge(IR_range_status_mean, how='left').merge(LOI_range_status_mean, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (평균 리워드 포인트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패널 ID 별 획득 평균 포인트\n",
    "Points_earned_mean = pd.DataFrame(train.query('STATUS==1').groupby('userID')['CPI'].agg([('Points_earned_mean',np.mean)])).reset_index()\n",
    "\n",
    "train = train.merge(Points_earned_mean, how='left')\n",
    "train['Points_earned_mean'].fillna(0, inplace = True)\n",
    "test = test.merge(Points_earned_mean, how='left')\n",
    "test['Points_earned_mean'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (TITLE 정규표현식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [^ ㄱ-ㅣ가-힣] 정규 표현식\n",
    "# 다소 시간이 오래 걸림\n",
    "word_counts = {}\n",
    "def count_word(x):\n",
    "    if x['STATUS'] == 1:\n",
    "        for w in re.sub(r'[^ ㄱ-ㅣ가-힣]', '', x['TITLE']).split():\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "def score_word(x):\n",
    "    score = 0\n",
    "    for w in re.sub(r'[^ ㄱ-ㅣ가-힣]', '', x['TITLE']).split():\n",
    "        score += word_counts.get(w, 0)\n",
    "    return score    \n",
    "            \n",
    "train.apply(count_word, axis=1)\n",
    "train.TITLE = train.apply(score_word, axis=1)\n",
    "test.TITLE = test.apply(score_word, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (TYPE 별 응답확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_mean = pd.DataFrame(train.groupby('TYPE')['STATUS'].agg([('TYPE_mean',np.mean)])).reset_index()\n",
    "train = train.merge(TYPE_mean, how='left')\n",
    "test = test.merge(TYPE_mean, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (유저 아이디 별 응답확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond = train.query('STATUS == 1').groupby('userID')['userID'].agg([('respond',np.size)]).reset_index()\n",
    "total = train.groupby('userID')['userID'].agg([('total',np.size)]).reset_index()\n",
    "\n",
    "train = train.merge(respond).merge(total)\n",
    "\n",
    "train['Response_Probability'] = train['respond'] / train['total']\n",
    "test = pd.merge(test, train.drop_duplicates(subset=['userID'])[['userID', 'Response_Probability']], how='left')\n",
    "test['Response_Probability'] = test['Response_Probability'].fillna(0)\n",
    "\n",
    "del train['respond'], train['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1st Round 1,2,3등의 모든 피쳐를 추가하고 섞어보며 각각 점수를 확인한 결과, 1등과 3등의 피쳐를 적절히 섞는 방식이 가장 점수가 높았음.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3870e7ee60e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'STATUS == 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GENDER'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'g_i'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFREQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "res_freq = train.query('STATUS == 1').groupby('GENDER')['IR'].agg([('g_i','mean')]).reset_index()\n",
    "train = pd.merge(train, res_freq, how='left')\n",
    "test = pd.merge(test, res_freq, how='left')\n",
    "test.FREQ = test.g_i.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_freq = train.query('STATUS == 1').groupby('userID')['userID'].agg([('FREQ',np.size)]).reset_index()\n",
    "send_freq = train.groupby('userID')['userID'].agg([('SEND',np.size)]).reset_index()\n",
    "train = pd.merge(train, res_freq, how='left')\n",
    "train = pd.merge(train, send_freq, how='left')\n",
    "train['RESRATE'] = train['FREQ'] / train['SEND']\n",
    "del train['FREQ'], train['SEND']\n",
    "test = pd.merge(test, train.drop_duplicates(subset=['userID'])[['userID', 'RESRATE']], how='left')\n",
    "test['RESRATE'] = test['RESRATE'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TIME = train.TIME.astype('datetime64')\n",
    "test.TIME = test.TIME.astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TIME_hour'] = train.TIME.dt.hour\n",
    "train['TIME_min'] = train.TIME.dt.minute\n",
    "train['dayofweek'] = train.TIME.dt.dayofweek\n",
    "\n",
    "test['TIME_hour'] = test.TIME.dt.hour\n",
    "test['TIME_min'] = test.TIME.dt.minute\n",
    "test['dayofweek'] = test.TIME.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 무의미한 feature 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['STATUS']\n",
    "train.drop('STATUS', axis=1, inplace=True)\n",
    "\n",
    "test_id = test.iloc[:,0:1]\n",
    "test.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "#성능 개선에 무의미한 Feature 제거\n",
    "train.drop(['userID', 'surveyID', 'TIME', 'BIRTH', 'HOUR_range', 'CPI_range'], axis=1, inplace=True)\n",
    "test.drop(['userID', 'surveyID', 'TIME', 'BIRTH', 'HOUR_range', 'CPI_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 병합 후 결측값 처리\n",
    "features = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "features = features.astype({'GENDER':'object'})\n",
    "\n",
    "cat = features.select_dtypes(include=['object','category']).columns.to_list()# 범주형 변수\n",
    "num = features.select_dtypes(exclude=['object','category']).columns.to_list()# 수치형 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거 전 수치형 변수 시각화\n",
    "plt.figure(figsize = (8, 4))\n",
    "features[num].boxplot()\n",
    "plt.xticks(fontsize = 10, rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 이상치 제거\n",
    "features[num] = features[num].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 이상치 제거 후 수치형 변수 시각화\n",
    "plt.figure(figsize = (8, 4))\n",
    "features[num].boxplot()\n",
    "plt.xticks(fontsize = 10, rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 스케일링 전 train, test 분할\n",
    "X_train = features.iloc[:y_train.shape[0], :]\n",
    "X_test = features.iloc[y_train.shape[0]:, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[num] = scaler.fit_transform(X_train[num])\n",
    "X_test[num] = scaler.transform(X_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩\n",
    "df_encoded = pd.get_dummies(pd.concat([X_train, X_test]), columns=cat)\n",
    "X_train = df_encoded[:X_train.shape[0]]\n",
    "X_test = df_encoded[X_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 피쳐 셀렉션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method: Using SHAP values \n",
    "import shap\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# DF, based on which importance is checked\n",
    "X_importance = X_test\n",
    "\n",
    "# Explain model predictions using shap library:\n",
    "model = LGBMClassifier(random_state=0).fit(X_train, y_train)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_importance)\n",
    "\n",
    "# Plot summary_plot as barplot:\n",
    "shap.summary_plot(shap_values, X_importance, plot_type='bar')\n",
    "\n",
    "shap_sum = np.abs(shap_values).mean(axis=1)[1,:]\n",
    "importance_df = pd.DataFrame([X_importance.columns.tolist(), shap_sum.tolist()]).T\n",
    "importance_df.columns = ['column_name', 'shap_importance']\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 중요도가 0.1 이상\n",
    "SHAP_THRESHOLD = 0.1\n",
    "features_selected = importance_df.query('shap_importance > @SHAP_THRESHOLD').column_name.tolist()\n",
    "X_train = X_train[features_selected]\n",
    "X_test = X_test[features_selected]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test size 올리기위해 0.2\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=77) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.딥러닝 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 랜덤시드 1,2,3 고정 후 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매번 모델링을 할 때마다 동일한 결과를 얻기 위해 랜덤 시드 설정 동일하게 유지\n",
    "# 럭키 시드 찾는건 시간상 어려움\n",
    "\n",
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(8, activation='relu')(input) # 은닉층 낮추기\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "DEEP = keras.Model(input, output)\n",
    "\n",
    "DEEP.summary()\n",
    "\n",
    "DEEP.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc', keras.metrics.AUC()])\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5),\n",
    "             keras.callbacks.ModelCheckpoint(filepath='best_nn_model.h5', monitor='val_loss', save_best_only=True)] # patience 낮추기\n",
    "\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "hist = DEEP.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                 batch_size=128, epochs=150, callbacks=[callbacks, mc], shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ROC-AUC\n",
    "plt.plot(hist.history[\"auc\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_auc\"], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.title(\"ROC-AUC\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"dnn_submission_{t.month:02}{t.day:02}{t.hour:02}{t.minute:02}.csv\"\n",
    "pd.DataFrame({'ID': test_id[\"ID\"], 'STATUS': DEEP.predict(X_test).flatten()}).to_csv(fname, index=False)\n",
    "print(f\"'{fname}' is ready to submit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_val, y_val, X_test, test_id = pd.read_pickle('comp_data(SHAP = 0.1이상).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_seeds()함수를 아래와 같이 수정해야 함.\n",
    "def reset_seeds(SEED, reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.compat.v1.set_random_seed(SEED)\n",
    "#    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 저장할 폴더 생성\n",
    "folder = 'Ensemble'\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(20)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    reset_seeds(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = keras.Input(shape=(X_train.shape[1],))\n",
    "    x = keras.layers.Dense(8, activation='relu')(input)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(input, output)    \n",
    "\n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc', keras.metrics.AUC()])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=128, epochs=150, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"ID\": test_id.ID, \n",
    "        \"STATUS\": model.predict(X_test).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/loop_submission_{t.month:02}{t.day:02}_{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = 0\n",
    "for f in os.listdir(folder):\n",
    "    ext = os.path.splitext(f)[-1]\n",
    "    if ext == '.csv': \n",
    "        s = pd.read_csv(folder+\"/\"+f)\n",
    "    else: \n",
    "        continue\n",
    "    if len(s.columns) !=2:\n",
    "        continue\n",
    "    if nf == 0: \n",
    "        slist = s\n",
    "    else: \n",
    "        slist = pd.merge(slist, s, on=\"ID\")\n",
    "    nf += 1\n",
    "\n",
    "p = 1.5 # 이 값에 따라 성능이 달라짐 (p=0: 기하평균, p=1: 산술평균)    \n",
    "if nf >= 2:\n",
    "    if p == 0: \n",
    "        pred = 1\n",
    "        for j in range(nf): pred = pred * slist.iloc[:,j+1]\n",
    "        pred = pred**(1/nf)\n",
    "    else:\n",
    "        pred = 0\n",
    "        for j in range(nf): pred = pred + slist.iloc[:,j+1]**p\n",
    "        pred = pred / nf\n",
    "        pred = pred**(1/p)\n",
    "    submission = pd.DataFrame({'ID': slist.ID, 'STATUS': pred})\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"p{p}mean_submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "    submission.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seeds(reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  # optional\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(2)\n",
    "    tf.compat.v1.set_random_seed(3)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # for GPU\n",
    "    print(\"RANDOM SEEDS RESET\")  # optional\n",
    "   \n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv(os.path.abspath(\"../submissions\") + '/p1.5mean_submission_1214_1414.csv').to_numpy()[:, 1:]\n",
    "df_2 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89319_dnn_submission_12130240.csv').to_numpy()[:, 1:]\n",
    "df_3 = pd.read_csv(os.path.abspath(\"../submissions\") + '/p20mean_submission_1214_0345.csv').to_numpy()[:, 1:]\n",
    "df_4 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89255_dnn_submission_12130026.csv').to_numpy()[:, 1:]\n",
    "df_5 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89213_dnn_submission_12100106.csv').to_numpy()[:, 1:]\n",
    "df_6 = pd.read_csv(os.path.abspath(\"../submissions\") + '/1st_submission.csv').to_numpy()[:, 1:]\n",
    "df_7 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89198_dnn_submission_12122349.csv').to_numpy()[:, 1:]\n",
    "df_8 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89180_dnn_submission_12130053.csv').to_numpy()[:, 1:]\n",
    "df_9 = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\Competition\\notebooks\\0.89419_dnn_submission_12140036(submission ensemble).csv').to_numpy()[:, 1:]\n",
    "df_10 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89175_dnn_submission_12130142.csv').to_numpy()[:, 1:]\n",
    "df_11 = pd.read_csv(os.path.abspath(\"../submissions\") + '/p14mean_submission_1214_0318.csv').to_numpy()[:, 1:]\n",
    "df_12 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89169_dnn_submission_12111512.csv').to_numpy()[:, 1:]\n",
    "df_13 = pd.read_csv(os.path.abspath(\"../submissions\") + '/2nd_submission.csv').to_numpy()[:, 1:]\n",
    "df_14 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89148_dnn_submission_12120154.csv').to_numpy()[:, 1:]\n",
    "df_15 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89097_dnn_submission_12130301.csv').to_numpy()[:, 1:]\n",
    "df_16 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.89092_dnn_submission_12120037.csv').to_numpy()[:, 1:]\n",
    "df_17 = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\Competition\\notebooks\\0.89387_p1.5mean_submission_1214_1414.csv').to_numpy()[:, 1:]\n",
    "# df_18 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88975_dnn_submission_12120201.csv').to_numpy()[:, 1:]\n",
    "# df_19 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88954_dnn_submission_12111947.csv').to_numpy()[:, 1:]\n",
    "df_20 = pd.read_csv(os.path.abspath(\"../submissions\") + '/3rd_submission.csv').to_numpy()[:, 1:]\n",
    "# df_21 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88947_dnn_submission_12111458.csv').to_numpy()[:, 1:]\n",
    "# df_22 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88849_dnn_submission_12120123.csv').to_numpy()[:, 1:]\n",
    "# df_23 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88845_dnn_submission_12100247.csv').to_numpy()[:, 1:]\n",
    "# df_24 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88828_dnn_submission_12120108.csv').to_numpy()[:, 1:]\n",
    "df_25 = pd.read_csv(os.path.abspath(\"../submissions\") + '/p1.5mean_submission_1214_0344.csv').to_numpy()[:, 1:]\n",
    "# df_26 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88563_dnn_submission_12120220.csv').to_numpy()[:, 1:]\n",
    "# df_27 = pd.read_csv(os.path.abspath(\"../submissions\") + '/0.88511_dnn_submission_12111837.csv').to_numpy()[:, 1:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 0.8 * df_submit + 0.2 * df_2\n",
    "pred = 0.6 * pred + 0.4 * df_3\n",
    "pred = 0.6 * pred + 0.4 * df_4\n",
    "pred = 0.6 * pred + 0.4 * df_5\n",
    "pred = 0.6 * pred + 0.4 * df_6\n",
    "pred = 0.6 * pred + 0.4 * df_7\n",
    "pred = 0.6 * pred + 0.4 * df_8\n",
    "pred = 0.6 * pred + 0.4 * df_9\n",
    "pred = 0.6 * pred + 0.4 * df_10\n",
    "pred = 0.6 * pred + 0.4 * df_11\n",
    "pred = 0.6 * pred + 0.4 * df_12\n",
    "pred = 0.6 * pred + 0.4 * df_13\n",
    "pred = 0.6 * pred + 0.4 * df_14\n",
    "pred = 0.6 * pred + 0.4 * df_15\n",
    "pred = 0.6 * pred + 0.4 * df_16\n",
    "pred = 0.6 * pred + 0.4 * df_17\n",
    "# pred = 0.6 * pred + 0.4 * df_18\n",
    "# pred = 0.6 * pred + 0.4 * df_19\n",
    "pred = 0.6 * pred + 0.4 * df_20\n",
    "# pred = 0.6 * pred + 0.4 * df_21\n",
    "# pred = 0.6 * pred + 0.4 * df_22\n",
    "# pred = 0.6 * pred + 0.4 * df_23\n",
    "# pred = 0.6 * pred + 0.4 * df_24\n",
    "pred = 0.6 * pred + 0.4 * df_25\n",
    "# pred = 0.6 * pred + 0.4 * df_26\n",
    "# pred = 0.6 * pred + 0.4 * df_27\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Timestamp.now()\n",
    "fname = f\"dnn_submission_{t.month:02}{t.day:02}{t.hour:02}{t.minute:02}.csv\"\n",
    "pd.DataFrame({'ID': test_id[\"ID\"], 'STATUS': pred[:,0]}).to_csv(fname, index=False)\n",
    "print(f\"'{fname}' is ready to submit.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
